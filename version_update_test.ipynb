{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ffc45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Updating checkpoint: C:\\Users\\alamz\\Desktop\\Servers\\zenodofiles\\Models\\EquiNet\\ANTOINE-NRTL\\Without Self Activity\\model.pt ===\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n",
      "Loading pretrained parameter \"readout.10.weight\".\n",
      "Loading pretrained parameter \"readout.10.bias\".\n",
      "Loading pretrained parameter \"readout.13.weight\".\n",
      "Loading pretrained parameter \"readout.13.bias\".\n",
      "Loading pretrained parameter \"readout.16.weight\".\n",
      "Loading pretrained parameter \"readout.16.bias\".\n",
      "Loading pretrained parameter \"intrinsic_vp.1.weight\".\n",
      "Loading pretrained parameter \"intrinsic_vp.1.bias\".\n",
      "Loading pretrained parameter \"intrinsic_vp.4.weight\".\n",
      "Loading pretrained parameter \"intrinsic_vp.4.bias\".\n",
      "Loading pretrained parameter \"intrinsic_vp.7.weight\".\n",
      "Loading pretrained parameter \"intrinsic_vp.7.bias\".\n",
      "Loading pretrained parameter \"intrinsic_vp.10.weight\".\n",
      "Loading pretrained parameter \"intrinsic_vp.10.bias\".\n",
      "Loading pretrained parameter \"intrinsic_vp.13.weight\".\n",
      "Loading pretrained parameter \"intrinsic_vp.13.bias\".\n",
      "Loading pretrained parameter \"intrinsic_vp.16.weight\".\n",
      "Loading pretrained parameter \"intrinsic_vp.16.bias\".\n",
      "  args.version:  None -> '0.2.0'\n",
      "  model.version: None -> '0.2.0'\n",
      "  Backup written to C:\\Users\\alamz\\Desktop\\Servers\\zenodofiles\\Models\\EquiNet\\ANTOINE-NRTL\\Without Self Activity\\model.pt.bak\n",
      "  ‚úÖ Checkpoint updated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from equinet.utils import (\n",
    "    load_args,\n",
    "    load_checkpoint,\n",
    "    load_scalers,\n",
    "    save_checkpoint,\n",
    ")\n",
    "\n",
    "# üîÅ Set the version you want to retro-tag with\n",
    "TARGET_VERSION = \"0.2.0\"\n",
    "\n",
    "# üìÇ Base directory where your .pt files live\n",
    "BASE_DIR = r\"C:\\Users\\alamz\\Desktop\\Servers\\zenodofiles\\Models\\EquiNet\\ANTOINE-NRTL\\Without Self Activity\"  # <-- change this\n",
    "\n",
    "\n",
    "def update_one_checkpoint(path: str, dry_run: bool = False):\n",
    "    print(f\"\\n=== Updating checkpoint: {path} ===\")\n",
    "\n",
    "    # 1) Load components\n",
    "    args = load_args(path)\n",
    "    model = load_checkpoint(path, device=None)  # adjust device arg if needed\n",
    "    scalers = load_scalers(path)  # should return list of 6 scalers\n",
    "\n",
    "    # 2) Set versions on args and model\n",
    "    old_args_version = getattr(args, \"version\", None)\n",
    "    old_model_version = getattr(model, \"version\", None)\n",
    "\n",
    "    args.version = TARGET_VERSION\n",
    "    setattr(model, \"version\", TARGET_VERSION)\n",
    "\n",
    "    print(f\"  args.version:  {old_args_version!r} -> {args.version!r}\")\n",
    "    print(f\"  model.version: {old_model_version!r} -> {model.version!r}\")\n",
    "\n",
    "    if dry_run:\n",
    "        print(\"  [DRY RUN] Not writing changes.\")\n",
    "        return\n",
    "\n",
    "    # 3) Make a backup\n",
    "    backup_path = path + \".bak\"\n",
    "    if not os.path.exists(backup_path):\n",
    "        shutil.copy2(path, backup_path)\n",
    "        print(f\"  Backup written to {backup_path}\")\n",
    "    else:\n",
    "        print(f\"  Backup already exists at {backup_path}, not overwriting.\")\n",
    "\n",
    "    # 4) Save updated checkpoint\n",
    "    # save_checkpoint(path, model, scaler1, scaler2, ..., args)\n",
    "    # scalers is expected to be a list of 6 scalers.\n",
    "    save_checkpoint(path, model, *scalers, args)\n",
    "    print(\"  ‚úÖ Checkpoint updated.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # üîç 1) FIRST: test on a single file you don‚Äôt care about\n",
    "    # For safety, set SINGLE_PATH to a specific file and run once.\n",
    "    SINGLE_PATH = None  # e.g. \"/home/.../model_0/test_model.pt\"\n",
    "\n",
    "    if SINGLE_PATH is not None:\n",
    "        update_one_checkpoint(SINGLE_PATH, dry_run=False)\n",
    "        return\n",
    "\n",
    "    # üîÅ 2) Once you're confident, walk a tree of checkpoints\n",
    "    for root, _, files in os.walk(BASE_DIR):\n",
    "        for fname in files:\n",
    "            if not fname.endswith(\".pt\"):\n",
    "                continue\n",
    "            full_path = os.path.join(root, fname)\n",
    "            update_one_checkpoint(full_path, dry_run=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1519c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"intrinsic_vp.1.weight\".\n",
      "Loading pretrained parameter \"intrinsic_vp.1.bias\".\n",
      "Loading pretrained parameter \"intrinsic_vp.4.weight\".\n",
      "Loading pretrained parameter \"intrinsic_vp.4.bias\".\n",
      "args.version: None\n",
      "model.version: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from equinet.utils import load_args, load_checkpoint\n",
    "\n",
    "path = r\"C:\\Users\\alamz\\Desktop\\Servers\\zenodofiles\\test\\model.pt\"\n",
    "args = load_args(path)\n",
    "model = load_checkpoint(path, device=None)\n",
    "\n",
    "print(\"args.version:\", getattr(args, \"version\", None))\n",
    "print(\"model.version:\", getattr(model, \"version\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e55d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: C:\\Users\\alamz\\Desktop\\Servers\\zenodofiles\\test\\model.pt\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.1.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"intrinsic_vp.1.weight\".\n",
      "Loading pretrained parameter \"intrinsic_vp.1.bias\".\n",
      "Loading pretrained parameter \"intrinsic_vp.4.weight\".\n",
      "Loading pretrained parameter \"intrinsic_vp.4.bias\".\n",
      "BEFORE:\n",
      "  args.version  : <no attr>\n",
      "  model.version : <no attr>\n",
      "AFTER setting:\n",
      "  args.version  : 0.2.0\n",
      "  model.version : 0.2.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_checkpoint() got an unexpected keyword argument 'phase_features_scaler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  model.version :\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mversion)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Re-save\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m save_checkpoint(\n\u001b[0;32m     33\u001b[0m     path\u001b[38;5;241m=\u001b[39mckpt_path,\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     35\u001b[0m     scaler\u001b[38;5;241m=\u001b[39mscalers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scalers, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m scalers,\n\u001b[0;32m     36\u001b[0m     features_scaler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m     atom_descriptor_scaler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m     bond_descriptor_scaler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     39\u001b[0m     phase_features_scaler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Checkpoint updated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: save_checkpoint() got an unexpected keyword argument 'phase_features_scaler'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from equinet.utils import load_args, load_checkpoint, load_scalers, save_checkpoint\n",
    "\n",
    "# <-- put the exact path you used in your tester here\n",
    "ckpt_path = r\"C:\\Users\\alamz\\Desktop\\Servers\\zenodofiles\\test\\model.pt\"\n",
    "\n",
    "print(\"Loading from:\", ckpt_path)\n",
    "args = load_args(ckpt_path)\n",
    "model = load_checkpoint(ckpt_path, device=None)\n",
    "scalers = load_scalers(ckpt_path)\n",
    "\n",
    "print(\"BEFORE:\")\n",
    "print(\"  args.version  :\", getattr(args, \"version\", \"<no attr>\"))\n",
    "print(\"  model.version :\", getattr(model, \"version\", \"<no attr>\"))\n",
    "\n",
    "# Manually set version\n",
    "if not hasattr(args, \"version\"):\n",
    "    args.version = \"0.2.0\"\n",
    "if not hasattr(model, \"version\"):\n",
    "    model.version = \"0.2.0\"\n",
    "\n",
    "if args.version is None:\n",
    "    args.version = \"0.2.0\"\n",
    "if model.version is None:\n",
    "    model.version = \"0.2.0\"\n",
    "\n",
    "print(\"AFTER setting:\")\n",
    "print(\"  args.version  :\", args.version)\n",
    "print(\"  model.version :\", model.version)\n",
    "\n",
    "# Re-save\n",
    "save_checkpoint(\n",
    "    path=ckpt_path,\n",
    "    model=model,\n",
    "    scaler=scalers[0] if isinstance(scalers, (list, tuple)) else scalers,\n",
    "    features_scaler=None,\n",
    "    atom_descriptor_scaler=None,\n",
    "    bond_descriptor_scaler=None,\n",
    "    phase_features_scaler=None,\n",
    "    args=args,\n",
    ")\n",
    "print(\"‚úÖ Checkpoint updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
